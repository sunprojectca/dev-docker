{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "# Redis Click Stream → XGBoost Demo\n",
        "\n",
        "Steps:\n",
        "1. Connect to local Redis (service name `redis`).\n",
        "2. Generate synthetic click records (logic adapted from `fastapi/generator.py`).\n",
        "3. Store each record in Redis as a JSON document (`click:<index>`).\n",
        "4. Read records back from Redis and build a DataFrame.\n",
        "5. Train an XGBoost classifier to predict `y_fraud`.\n",
        "6. Display evaluation metrics and feature importances.\n",
        "\n",
        "**Note:** The original `generator.py` isn't mounted inside the Jupyter container (not in docker-compose volumes). We inline a trimmed, parameterized version of its logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Packages ready.\n"
          ]
        }
      ],
      "source": [
        "# Install needed packages (idempotent).\n",
        "import sys, subprocess, importlib\n",
        "def ensure(pkg):\n",
        "    try: importlib.import_module(pkg)\n",
        "    except ImportError: subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg])\n",
        "for p in ['redis', 'xgboost', 'pandas', 'numpy', 'scikit-learn']:\n",
        "    ensure(p)\n",
        "print('Packages ready.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in c:\\users\\admin_tonyt\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.10.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\admin_tonyt\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\admin_tonyt\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\admin_tonyt\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\admin_tonyt\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in c:\\users\\admin_tonyt\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (2.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin_tonyt\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\admin_tonyt\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\admin_tonyt\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\admin_tonyt\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\admin_tonyt\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Fix missing matplotlib\n",
        "%pip install matplotlib\n",
        "\n",
        "# Imports & configuration\n",
        "import os, math, random, string, ipaddress, uuid, numpy as np, pandas as pd, time, json\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from redis import Redis\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "random.seed(7); np.random.seed(7)\n",
        "REDIS_HOST = os.getenv('REDIS_HOST','redis')\n",
        "REDIS_PORT = int(os.getenv('REDIS_PORT','6379'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected to Redis via host 'localhost'\n"
          ]
        }
      ],
      "source": [
        "import os, socket\n",
        "from redis import Redis\n",
        "\n",
        "CANDIDATE_HOSTS = [\n",
        "    os.getenv(\"REDIS_HOST\", \"redis\"),\n",
        "    \"localhost\",\n",
        "    \"127.0.0.1\"\n",
        "]\n",
        "\n",
        "redis_host = None\n",
        "for h in CANDIDATE_HOSTS:\n",
        "    try:\n",
        "        socket.gethostbyname(h)\n",
        "        r = Redis(host=h, port=6379, decode_responses=True, socket_connect_timeout=2)\n",
        "        if r.ping():\n",
        "            redis_host = h\n",
        "            print(f\"Connected to Redis via host '{h}'\")\n",
        "            break\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "if not redis_host:\n",
        "    raise SystemExit(\"Could not connect to Redis using any host in: \" + \", \".join(CANDIDATE_HOSTS))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## Data Generation\n",
        "Trimmed synthetic clickstream similar to the original script. Parameters below let you reduce size for quick iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN_tonyt\\AppData\\Local\\Temp\\ipykernel_1660\\3937299601.py:92: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  start_day = (datetime.utcnow() - timedelta(days=days-1)).date()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated rows: 5013\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>event_ts</th>\n",
              "      <th>event_date</th>\n",
              "      <th>hour_of_day</th>\n",
              "      <th>source_type</th>\n",
              "      <th>farm</th>\n",
              "      <th>search_query</th>\n",
              "      <th>query_intent_category</th>\n",
              "      <th>is_brand_query</th>\n",
              "      <th>rank_position</th>\n",
              "      <th>serp_impressions</th>\n",
              "      <th>...</th>\n",
              "      <th>revenue</th>\n",
              "      <th>ip</th>\n",
              "      <th>ip_subnet24</th>\n",
              "      <th>isp_class</th>\n",
              "      <th>headless</th>\n",
              "      <th>y_fraud</th>\n",
              "      <th>y_conv</th>\n",
              "      <th>ctr</th>\n",
              "      <th>cvr</th>\n",
              "      <th>roas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1755475267000</td>\n",
              "      <td>2025-08-18</td>\n",
              "      <td>0</td>\n",
              "      <td>human</td>\n",
              "      <td>None</td>\n",
              "      <td>discount gym</td>\n",
              "      <td>transactional</td>\n",
              "      <td>False</td>\n",
              "      <td>22</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>residential</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1755475440000</td>\n",
              "      <td>2025-08-18</td>\n",
              "      <td>0</td>\n",
              "      <td>bot</td>\n",
              "      <td>farm1</td>\n",
              "      <td>insurance amazon</td>\n",
              "      <td>informational</td>\n",
              "      <td>False</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>52.23.10.61</td>\n",
              "      <td>52.23.10.0/24</td>\n",
              "      <td>datacenter</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1755475476000</td>\n",
              "      <td>2025-08-18</td>\n",
              "      <td>0</td>\n",
              "      <td>bot</td>\n",
              "      <td>farm1</td>\n",
              "      <td>gym wikipedia</td>\n",
              "      <td>informational</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>52.23.30.96</td>\n",
              "      <td>52.23.30.0/24</td>\n",
              "      <td>datacenter</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1755475543000</td>\n",
              "      <td>2025-08-18</td>\n",
              "      <td>0</td>\n",
              "      <td>bot</td>\n",
              "      <td>farm1</td>\n",
              "      <td>tutorial coffee</td>\n",
              "      <td>informational</td>\n",
              "      <td>False</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>52.23.30.80</td>\n",
              "      <td>52.23.30.0/24</td>\n",
              "      <td>datacenter</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1755475550000</td>\n",
              "      <td>2025-08-18</td>\n",
              "      <td>0</td>\n",
              "      <td>human</td>\n",
              "      <td>None</td>\n",
              "      <td>credit card amazon</td>\n",
              "      <td>informational</td>\n",
              "      <td>False</td>\n",
              "      <td>26</td>\n",
              "      <td>9</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>residential</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1111</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 36 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        event_ts  event_date  hour_of_day source_type   farm  \\\n",
              "0  1755475267000  2025-08-18            0       human   None   \n",
              "1  1755475440000  2025-08-18            0         bot  farm1   \n",
              "2  1755475476000  2025-08-18            0         bot  farm1   \n",
              "3  1755475543000  2025-08-18            0         bot  farm1   \n",
              "4  1755475550000  2025-08-18            0       human   None   \n",
              "\n",
              "         search_query query_intent_category  is_brand_query  rank_position  \\\n",
              "0        discount gym         transactional           False             22   \n",
              "1    insurance amazon         informational           False             18   \n",
              "2       gym wikipedia         informational           False              2   \n",
              "3     tutorial coffee         informational           False             13   \n",
              "4  credit card amazon         informational           False             26   \n",
              "\n",
              "   serp_impressions  ...  revenue           ip    ip_subnet24    isp_class  \\\n",
              "0                 4  ...      0.0         None           None  residential   \n",
              "1                 2  ...      0.0  52.23.10.61  52.23.10.0/24   datacenter   \n",
              "2                 5  ...      0.0  52.23.30.96  52.23.30.0/24   datacenter   \n",
              "3                 2  ...      0.0  52.23.30.80  52.23.30.0/24   datacenter   \n",
              "4                 9  ...      0.0         None           None  residential   \n",
              "\n",
              "  headless y_fraud y_conv     ctr  cvr  roas  \n",
              "0    False       0      0  0.2500  0.0   0.0  \n",
              "1     True       1      0  0.0000  0.0   0.0  \n",
              "2     True       1      0  0.4000  0.0   0.0  \n",
              "3     True       1      0  0.0000  0.0   0.0  \n",
              "4    False       0      0  0.1111  0.0   0.0  \n",
              "\n",
              "[5 rows x 36 columns]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Parameter knobs (high-volume defaults to reach ~100k rows). Adjust as needed.\n",
        "DAYS = 20                 # base days to simulate first (can be extended automatically)\n",
        "HUMANS_PER_DAY = 2000     # expected mean humans per day\n",
        "BOTS_PER_DAY = 3000       # expected mean bots per day (total across all farms)\n",
        "N_FARMS = 3\n",
        "VALUE_PER_CONV = 300.0\n",
        "TARGET_ROWS = 100_000     # If not None, keep simulating extra days until >= this many rows\n",
        "MAX_EXTRA_DAYS = 30       # Safety cap to avoid runaway generation\n",
        "\n",
        "# Helper utils adapted from generator.py\n",
        "def wchoice(pairs):\n",
        "    items, weights = zip(*pairs)\n",
        "    return random.choices(items, weights=weights, k=1)[0]\n",
        "\n",
        "QUERY_CATEGORIES = [(\"navigational\", 0.28), (\"informational\", 0.44), (\"transactional\", 0.23), (\"local\", 0.05)]\n",
        "DEVICES = [(\"desktop\", 0.46), (\"mobile\", 0.47), (\"tablet\", 0.07)]\n",
        "COUNTRIES = [(\"US\",0.46),(\"GB\",0.10),(\"CA\",0.09),(\"AU\",0.06),(\"IN\",0.12),(\"DE\",0.06),(\"FR\",0.05),(\"BR\",0.04),(\"JP\",0.02)]\n",
        "LANGS = [(\"en\",0.55),(\"en-US\",0.20),(\"en-GB\",0.10),(\"de\",0.04),(\"fr\",0.04),(\"pt\",0.03),(\"hi\",0.02),(\"ja\",0.01),(\"es\",0.01)]\n",
        "BROWSERS = [(\"Chrome\",0.62),(\"Safari\",0.18),(\"Edge\",0.10),(\"Firefox\",0.08),(\"Other\",0.02)]\n",
        "OSES = [(\"Windows\",0.35),(\"Android\",0.32),(\"iOS\",0.20),(\"macOS\",0.10),(\"Linux\",0.03)]\n",
        "SERP_FEATURES = [\"none\",\"sitelinks\",\"faq\",\"video\",\"image\",\"news\",\"localpack\",\"shopping\"]\n",
        "CAMPAIGNS = [\"Brand\",\"Generic\",\"Competitor\",\"Retargeting\",\"DisplayProspecting\"]\n",
        "ADGROUPS = {\n",
        "    \"Brand\":[\"Brand Core\",\"Brand Location\"],\n",
        "    \"Generic\":[\"Apartments City\",\"Amenities\",\"Near Me\"],\n",
        "    \"Competitor\":[\"Comp A\",\"Comp B\"],\n",
        "    \"Retargeting\":[\"Visitors 7d\",\"Abandoned Tours\"],\n",
        "    \"DisplayProspecting\":[\"In-Market\",\"Affinity Home\"]\n",
        "}\n",
        "\n",
        "def random_query(category):\n",
        "    topics = {\n",
        "        \"navigational\": [\"facebook login\",\"youtube\",\"gmail\",\"amazon\",\"wikipedia\",\"bank\",\"official site\"],\n",
        "        \"informational\": [\"how to\",\"what is\",\"best way to\",\"benefits of\",\"guide to\",\"tutorial\"],\n",
        "        \"transactional\": [\"buy\",\"price\",\"discount\",\"near me\",\"coupon\",\"best\"],\n",
        "        \"local\": [\"near me\",\"closest\",\"open now\",\"hours\",\"directions\",\"map\"]\n",
        "    }\n",
        "    nouns = [\"apartments\",\"redis\",\"python\",\"laptop\",\"headphones\",\"router\",\"ssd\",\"credit card\",\"insurance\",\"pizza\",\"coffee\",\"gym\"]\n",
        "    t = random.choice(topics[category]); n = random.choice(nouns)\n",
        "    if category in (\"navigational\",\"local\"): return f\"{n} {t}\"\n",
        "    return f\"{t} {n}\"\n",
        "\n",
        "def positional_ctr(position):\n",
        "    base = 0.38 * (1 / math.log(position + 1.8))\n",
        "    noise = random.uniform(-0.03, 0.03)\n",
        "    return max(0.0, min(0.8, base + noise))\n",
        "\n",
        "def build_queries(n=600):  # more candidate queries for bigger dataset\n",
        "    cats = [wchoice(QUERY_CATEGORIES) for _ in range(n)]\n",
        "    q = [random_query(c) for c in cats]\n",
        "    q = list(dict.fromkeys(q))\n",
        "    idx = np.arange(1, len(q)+1)\n",
        "    zipf_w = (1 / (idx ** 1.1)); zipf_w /= zipf_w.sum()\n",
        "    return q, zipf_w\n",
        "\n",
        "def pick_campaign():\n",
        "    c = random.choice(CAMPAIGNS)\n",
        "    g = random.choice(ADGROUPS[c])\n",
        "    ad_id = random.randint(100000, 999999)\n",
        "    creative = random.choice([\"RSA\",\"Text\",\"Display\",\"Video\"])\n",
        "    return c, g, ad_id, creative\n",
        "\n",
        "def sample_bot_farms(n=2):\n",
        "    farms = []\n",
        "    for i in range(n):\n",
        "        farms.append({\n",
        "            'name': f'farm{i+1}',\n",
        "            'blocks': [f'52.{23+i}.{x}.0/24' for x in (10,20,30)],\n",
        "            'active_hours': sorted(random.sample(range(24), k=10)),\n",
        "            'ctr_multiplier': 0.6,\n",
        "            'cvr_multiplier': 0.05,\n",
        "            'bounce_bias': 0.4,\n",
        "            'headless_rate': 0.5\n",
        "        })\n",
        "    return farms\n",
        "\n",
        "def ipv4_from_block(block_cidr):\n",
        "    net = ipaddress.IPv4Network(block_cidr)\n",
        "    host = int(net.network_address) + random.randint(1, net.num_addresses-2)\n",
        "    return str(ipaddress.IPv4Address(host))\n",
        "\n",
        "def sample_bot_ip(farm):\n",
        "    block = random.choice(farm['blocks'])\n",
        "    ip = ipv4_from_block(block)\n",
        "    return ip, block\n",
        "\n",
        "def _simulate_one_day(day, queries, zipf_w, farms, BRAND_TERMS, ranks, rank_w, rows, humans_per_day, bots_per_day):\n",
        "    # Humans\n",
        "    n_h = np.random.poisson(humans_per_day)\n",
        "    for _ in range(n_h):\n",
        "        hour = random.randint(0,23)\n",
        "        dev = wchoice(DEVICES); country = wchoice(COUNTRIES); lang = wchoice(LANGS)\n",
        "        browser = wchoice(BROWSERS); os_name = wchoice(OSES)\n",
        "        q = random.choices(queries, weights=zipf_w, k=1)[0]\n",
        "        cat = ('transactional' if any(x in q for x in ['buy','price','discount','coupon']) else\n",
        "               'local' if any(x in q for x in ['near me','open now','hours','directions']) else\n",
        "               'navigational' if any(x in q for x in ['login','official','site']) else 'informational')\n",
        "        pos = int(np.random.choice(ranks, p=rank_w))\n",
        "        impr = max(1, int(np.random.lognormal(mean=1.3, sigma=0.5)))\n",
        "        base_ctr = positional_ctr(pos)\n",
        "        if dev=='mobile': base_ctr *= 0.95\n",
        "        is_brand = (q in BRAND_TERMS) or ('official' in q) or ('login' in q)\n",
        "        if is_brand: base_ctr = min(0.85, base_ctr*1.3)\n",
        "        clicks = np.random.binomial(impr, max(0, min(0.95, base_ctr)))\n",
        "        base_cvr = {'navigational':0.11,'informational':0.035,'transactional':0.08,'local':0.05}[cat]\n",
        "        conv = int(np.random.binomial(clicks, base_cvr)) if clicks>0 else 0\n",
        "        campaign, adgroup, ad_id, creative = pick_campaign()\n",
        "        cpc = max(0.1, np.random.normal(0.9, 0.3))\n",
        "        cost = round(cpc*clicks,2); revenue = round(conv*VALUE_PER_CONV*np.random.uniform(0.9,1.1),2)\n",
        "        dwell = 0 if clicks==0 else int(max(3, np.random.normal(60,20)))\n",
        "        bounce = (dwell < 30 and random.random()<0.4)\n",
        "        ts = datetime(day.year, day.month, day.day, hour, random.randint(0,59), random.randint(0,59), tzinfo=timezone.utc)\n",
        "        rows.append({\n",
        "            'event_ts': int(ts.timestamp()*1000),\n",
        "            'event_date': str(day),\n",
        "            'hour_of_day': hour,\n",
        "            'source_type': 'human',\n",
        "            'farm': None,\n",
        "            'search_query': q,\n",
        "            'query_intent_category': cat,\n",
        "            'is_brand_query': bool(is_brand),\n",
        "            'rank_position': pos,\n",
        "            'serp_impressions': impr,\n",
        "            'serp_clicks': clicks,\n",
        "            'click_through_rate': round(clicks/impr,4),\n",
        "            'device_type': dev,\n",
        "            'os': os_name,\n",
        "            'browser': browser,\n",
        "            'user_country': country,\n",
        "            'user_language': lang,\n",
        "            'campaign': campaign,\n",
        "            'ad_group': adgroup,\n",
        "            'ad_id': ad_id,\n",
        "            'creative_type': creative,\n",
        "            'dwell_time_seconds': dwell,\n",
        "            'bounced_session': bool(bounce),\n",
        "            'conversions': conv,\n",
        "            'cpc': float(round(cpc,2)),\n",
        "            'cost': float(cost),\n",
        "            'revenue': float(revenue),\n",
        "            'ip': None,\n",
        "            'ip_subnet24': None,\n",
        "            'isp_class': 'residential',\n",
        "            'headless': False,\n",
        "            'y_fraud': 0,\n",
        "            'y_conv': 1 if conv>0 else 0\n",
        "        })\n",
        "    # Bots\n",
        "    for farm in farms:\n",
        "        n_b = np.random.poisson(bots_per_day / max(1,len(farms)))\n",
        "        for _ in range(n_b):\n",
        "            hour = random.choice(farm['active_hours'])\n",
        "            dev = wchoice(DEVICES); country = wchoice(COUNTRIES); lang = wchoice(LANGS)\n",
        "            q = random.choices(queries, weights=zipf_w, k=1)[0]\n",
        "            cat = ('transactional' if any(x in q for x in ['buy','price','discount','coupon']) else\n",
        "                   'local' if any(x in q for x in ['near me','open now','hours','directions']) else\n",
        "                   'navigational' if any(x in q for x in ['login','official','site']) else 'informational')\n",
        "            pos = int(np.random.choice(ranks, p=rank_w))\n",
        "            impr = max(1, int(np.random.lognormal(mean=1.5, sigma=0.6)))\n",
        "            base_ctr = positional_ctr(pos) * 0.5\n",
        "            clicks = np.random.binomial(impr, max(0,min(0.9, base_ctr)))\n",
        "            conv = 0  # negligible\n",
        "            campaign, adgroup, ad_id, creative = pick_campaign()\n",
        "            cpc = max(0.05, np.random.normal(0.4,0.15))\n",
        "            cost = round(cpc*clicks,2); revenue = 0.0\n",
        "            ip, subnet = sample_bot_ip(farm)\n",
        "            dwell = 0 if clicks==0 else int(max(1, np.random.normal(8,4)))\n",
        "            bounce = True if dwell < 5 else False\n",
        "            ts = datetime(day.year, day.month, day.day, hour, random.randint(0,59), random.randint(0,59), tzinfo=timezone.utc)\n",
        "            rows.append({\n",
        "                'event_ts': int(ts.timestamp()*1000),\n",
        "                'event_date': str(day),\n",
        "                'hour_of_day': hour,\n",
        "                'source_type': 'bot',\n",
        "                'farm': farm['name'],\n",
        "                'search_query': q,\n",
        "                'query_intent_category': cat,\n",
        "                'is_brand_query': ('official' in q) or ('login' in q),\n",
        "                'rank_position': pos,\n",
        "                'serp_impressions': impr,\n",
        "                'serp_clicks': clicks,\n",
        "                'click_through_rate': round(clicks/impr,4),\n",
        "                'device_type': dev,\n",
        "                'os': wchoice(OSES)[0] if isinstance(wchoice(OSES), tuple) else dev,\n",
        "                'browser': 'Chrome',\n",
        "                'user_country': country,\n",
        "                'user_language': lang,\n",
        "                'campaign': campaign,\n",
        "                'ad_group': adgroup,\n",
        "                'ad_id': ad_id,\n",
        "                'creative_type': creative,\n",
        "                'dwell_time_seconds': dwell,\n",
        "                'bounced_session': bool(bounce),\n",
        "                'conversions': conv,\n",
        "                'cpc': float(round(cpc,2)),\n",
        "                'cost': float(cost),\n",
        "                'revenue': float(revenue),\n",
        "                'ip': ip,\n",
        "                'ip_subnet24': subnet,\n",
        "                'isp_class': 'datacenter',\n",
        "                'headless': True,\n",
        "                'y_fraud': 1,\n",
        "                'y_conv': 0\n",
        "            })\n",
        "\n",
        "def generate_dataset(days=DAYS, humans_per_day=HUMANS_PER_DAY, bots_per_day=BOTS_PER_DAY, n_farms=N_FARMS, target_rows: int | None = TARGET_ROWS):\n",
        "    queries, zipf_w = build_queries()\n",
        "    BRAND_TERMS = set(q for q in queries if 'official' in q or 'login' in q or 'gmail' in q)\n",
        "    ranks = np.arange(1, 31)\n",
        "    rank_w = np.linspace(0.3,0.7,len(ranks))[::-1]; rank_w /= rank_w.sum()\n",
        "    farms = sample_bot_farms(n_farms)\n",
        "    rows = []\n",
        "    start_day = (datetime.utcnow() - timedelta(days=days-1)).date()\n",
        "    # Initial fixed window\n",
        "    for d_off in range(days):\n",
        "        day = start_day + timedelta(days=d_off)\n",
        "        _simulate_one_day(day, queries, zipf_w, farms, BRAND_TERMS, ranks, rank_w, rows, humans_per_day, bots_per_day)\n",
        "    extra_days = 0\n",
        "    while target_rows and len(rows) < target_rows and extra_days < MAX_EXTRA_DAYS:\n",
        "        day = start_day + timedelta(days=days + extra_days)\n",
        "        _simulate_one_day(day, queries, zipf_w, farms, BRAND_TERMS, ranks, rank_w, rows, humans_per_day, bots_per_day)\n",
        "        extra_days += 1\n",
        "        if extra_days % 2 == 0:\n",
        "            print(f\"Extended generation: {extra_days} extra days, rows so far {len(rows):,}\")\n",
        "    if target_rows and len(rows) < target_rows:\n",
        "        print(f\"WARNING: Reached MAX_EXTRA_DAYS ({MAX_EXTRA_DAYS}) with only {len(rows):,} rows < target {target_rows:,}\")\n",
        "    df = pd.DataFrame(rows).sort_values('event_ts').reset_index(drop=True)\n",
        "    df['ctr'] = df['click_through_rate']\n",
        "    df['cvr'] = (df['conversions'] / df['serp_clicks'].replace(0, np.nan)).fillna(0.0)\n",
        "    df['roas'] = (df['revenue'] / df['cost'].replace(0, np.nan)).replace([np.inf,-np.inf],0).fillna(0.0)\n",
        "    print(f\"Generated rows: {len(df):,} across {df['event_date'].nunique()} days (base {days} + extra {extra_days})\")\n",
        "    return df\n",
        "\n",
        "df = generate_dataset()\n",
        "print('Final dataset size:', len(df))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d18af452",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Generation Debug ---\n",
            "Configured DAYS: 5 HUMANS_PER_DAY: 400 BOTS_PER_DAY: 600 N_FARMS: 2\n",
            "Actual df rows: 5013\n",
            "Row count looks reasonable; no anomaly detected.\n",
            "------------------------\n"
          ]
        }
      ],
      "source": [
        "# DEBUG: Diagnose unexpectedly low row count\n",
        "print('--- Generation Debug ---')\n",
        "print('Configured DAYS:', DAYS, 'HUMANS_PER_DAY:', HUMANS_PER_DAY, 'BOTS_PER_DAY:', BOTS_PER_DAY, 'N_FARMS:', N_FARMS)\n",
        "print('Actual df rows:', len(df))\n",
        "if len(df) < 50:\n",
        "    # Show per-day counts\n",
        "    if 'event_date' in df.columns:\n",
        "        print('Per-day row counts:')\n",
        "        print(df.groupby('event_date').size())\n",
        "    # Show first few unique queries and source types\n",
        "    print('Source type counts:')\n",
        "    print(df['source_type'].value_counts())\n",
        "    # Inspect sample of raw rows\n",
        "    print('Sample rows dicts:')\n",
        "    for i, rec in df.head(5).iterrows():\n",
        "        print(rec.to_dict())\n",
        "    # Re-run a manual Poisson draw to confirm RNG\n",
        "    import numpy as _np\n",
        "    print('Sample Poisson humans draw (lambda=HUMANS_PER_DAY):', _np.random.poisson(HUMANS_PER_DAY))\n",
        "else:\n",
        "    print('Row count looks reasonable; no anomaly detected.')\n",
        "print('------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## Write Records to Redis as JSON\n",
        "Each row → key `click:<row_index>` (JSON)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47fa80d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Always clear previous click:* keys to start fresh (fast SCAN + UNLINK)\n",
        "import time as _t\n",
        "prefix = 'click:'\n",
        "start_clear = _t.time()\n",
        "pipe_del = r.pipeline(transaction=False)\n",
        "count_del = 0\n",
        "for k in r.scan_iter(f\"{prefix}*\", count=1000):\n",
        "    pipe_del.unlink(k)  # non-blocking delete\n",
        "    count_del += 1\n",
        "    if count_del % 10000 == 0:\n",
        "        pipe_del.execute()\n",
        "        print(f\"Queued deletions: {count_del:,}...\")\n",
        "# flush remaining deletions\n",
        "pipe_del.execute()\n",
        "print(f\"Cleared {count_del:,} existing keys in {(_t.time()-start_clear):.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c26d1da1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify DB is empty for click:* before insertion\n",
        "remaining = sum(1 for _ in r.scan_iter('click:*'))\n",
        "print('Remaining click:* keys after clear:', remaining)\n",
        "assert remaining == 0, 'Expected zero click:* keys before insertion.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Flushed 1,000 keys...\n",
            "Flushed 2,000 keys...\n",
            "Flushed 3,000 keys...\n",
            "Flushed 4,000 keys...\n",
            "Inserted 4,965 keys in 0.43s (11541.9 keys/sec)\n",
            "Key click:0 not found; first available key: click:4573\n",
            "Flushed 4,000 keys...\n",
            "Inserted 4,965 keys in 0.43s (11541.9 keys/sec)\n",
            "Key click:0 not found; first available key: click:4573\n"
          ]
        }
      ],
      "source": [
        "import json, time\n",
        "BATCH_SIZE = 5000  # larger batch for speed\n",
        "prefix = \"click:\"\n",
        "\n",
        "start = time.time()\n",
        "pipe = r.pipeline(transaction=False)\n",
        "queued = 0\n",
        "inserted = 0\n",
        "TOTAL = len(df)\n",
        "log_every = 20000\n",
        "for i, row in df.iterrows():\n",
        "    pipe.set(f\"{prefix}{i}\", json.dumps(row.to_dict(), separators=(',',':')))\n",
        "    queued += 1\n",
        "    if queued % BATCH_SIZE == 0:\n",
        "        res = pipe.execute(); inserted += len(res)\n",
        "        # Lightweight progress\n",
        "        if inserted % log_every == 0 or inserted == TOTAL:\n",
        "            pct = (inserted / TOTAL)*100\n",
        "            print(f\"Inserted {inserted:,}/{TOTAL:,} ({pct:5.1f}%)\")\n",
        "# flush remainder\n",
        "if queued % BATCH_SIZE:\n",
        "    res = pipe.execute(); inserted += len(res)\n",
        "\n",
        "elapsed = time.time() - start\n",
        "print(f\"Inserted {inserted:,} keys in {elapsed:.2f}s ({inserted/elapsed:.1f} keys/sec)\")\n",
        "\n",
        "# Safe example fetch\n",
        "example = r.get(f\"{prefix}0\")\n",
        "if example:\n",
        "    print(\"Example key value snippet:\", example[:140], \"...\")\n",
        "else:\n",
        "    print(\"Key click:0 not found; first available key:\",\n",
        "          next(r.scan_iter(f\"{prefix}*\"), \"NONE\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0df29e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diagnostics: reconcile DataFrame rows vs Redis keys\n",
        "import socket\n",
        "expected = len(df)\n",
        "print('Expected rows (len(df)):', expected)\n",
        "print('Inserted counter variable:', inserted)\n",
        "\n",
        "# Count keys via pattern scan (efficient incremental)\n",
        "click_key_count = 0\n",
        "for _ in r.scan_iter(f\"{prefix}*\", count=1000):\n",
        "    click_key_count += 1\n",
        "print('Discovered click:* keys via scan_iter:', click_key_count)\n",
        "\n",
        "# Total keys in DB\n",
        "print('DBSIZE (all keys):', r.dbsize())\n",
        "\n",
        "# Sample a few missing indices if discrepancy\n",
        "if click_key_count < expected:\n",
        "    missing = []\n",
        "    # check first 50 indices only for speed\n",
        "    for i in range(min(50, expected)):\n",
        "        if not r.exists(f\"{prefix}{i}\"):\n",
        "            missing.append(i)\n",
        "    print('Example missing early indices (first 50 checked):', missing[:10])\n",
        "\n",
        "# Show server identity to ensure we connected where we inserted\n",
        "info = r.info()\n",
        "print('Connected server:', info.get('redis_version'), 'mode', info.get('redis_mode'), 'port', info.get('tcp_port'))\n",
        "print('Server role:', info.get('role'), 'process id:', info.get('process_id'))\n",
        "print('Instance run id:', info.get('run_id'))\n",
        "print('Current client name (if any):', r.client_info().get('name'))\n",
        "\n",
        "# Heuristic: warn if inserted << expected\n",
        "if inserted and expected and inserted < expected:\n",
        "    print('WARNING: pipeline reported fewer replies than DataFrame rows. Some commands may not have executed or were overwritten.')\n",
        "\n",
        "# Simple integrity check on a random sample of keys\n",
        "import random, json as _json\n",
        "if click_key_count:\n",
        "    sample_indices = random.sample(range(min(expected, click_key_count)), k=min(3, click_key_count))\n",
        "    for idx in sample_indices:\n",
        "        raw = r.get(f\"{prefix}{idx}\")\n",
        "        print(f'Sample key click:{idx} present:', bool(raw), 'size:', len(raw) if raw else 0)\n",
        "        if raw:\n",
        "            try:\n",
        "                rec = _json.loads(raw)\n",
        "                print('  Fields:', list(rec.keys())[:8], '...')\n",
        "            except Exception as e:\n",
        "                print('  JSON decode error:', e)\n",
        "else:\n",
        "    print('No click:* keys found in Redis.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## Read Back from Redis\n",
        "Scan keys with prefix `click:` and rebuild the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded rows: 0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: []"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def load_clicks(prefix='click:'):\n",
        "    cursor = 0; rows=[]\n",
        "    while True:\n",
        "        cursor, keys = r.scan(cursor=cursor, match=f'{prefix}*', count=500)\n",
        "        for k in keys:\n",
        "            raw = r.get(k)\n",
        "            if raw:\n",
        "                try: rows.append(json.loads(raw))\n",
        "                except json.JSONDecodeError: pass\n",
        "        if cursor == 0: break\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "df_loaded = load_clicks()\n",
        "print('Loaded rows:', len(df_loaded))\n",
        "df_loaded.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## Feature Engineering & Train/Test Split\n",
        "We'll predict `y_fraud` using a subset of features and simple one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"['y_fraud'] not found in axis\"",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m use_df.columns:\n\u001b[32m      7\u001b[39m         use_df[c] = use_df[c].astype(\u001b[33m'\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m X = pd.get_dummies(\u001b[43muse_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, drop_first=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      9\u001b[39m y = use_df[target].astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     10\u001b[39m X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=\u001b[32m0.25\u001b[39m, stratify=y, random_state=\u001b[32m42\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN_tonyt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:5588\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5440\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   5441\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5442\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5449\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5450\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5451\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5452\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5453\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5586\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5587\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5588\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5589\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5590\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5591\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5592\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5594\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5595\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5596\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN_tonyt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:4807\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4805\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4806\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4807\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4809\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4810\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN_tonyt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:4849\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4847\u001b[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001b[32m   4848\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4849\u001b[39m         new_axis = \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4850\u001b[39m     indexer = axis.get_indexer(new_axis)\n\u001b[32m   4852\u001b[39m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[32m   4853\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN_tonyt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7136\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7136\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7137\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
            "\u001b[31mKeyError\u001b[39m: \"['y_fraud'] not found in axis\""
          ]
        }
      ],
      "source": [
        "target = 'y_fraud'\n",
        "# Derive label if missing (e.g. if not persisted / older write missing column)\n",
        "if target not in df_loaded.columns:\n",
        "    print(f\"Target column '{target}' missing; deriving from source_type=='bot'.\")\n",
        "    if 'source_type' not in df_loaded.columns:\n",
        "        raise ValueError(\"Cannot derive y_fraud because 'source_type' column is absent and original target missing.\")\n",
        "    df_loaded[target] = (df_loaded['source_type'] == 'bot').astype(int)\n",
        "\n",
        "if df_loaded.empty:\n",
        "    raise ValueError(\"Loaded DataFrame is empty. Ensure Redis keys were inserted and load_clicks() worked.\")\n",
        "\n",
        "# Clean / reset index to avoid surprises\n",
        "use_df = df_loaded.reset_index(drop=True).copy()\n",
        "\n",
        "# Drop high-cardinality or unused columns (ignore if absent)\n",
        "drop_cols = ['search_query','event_date','ip','ip_subnet24']\n",
        "use_df = use_df.drop(columns=[c for c in drop_cols if c in use_df.columns])\n",
        "\n",
        "# Ensure target exists now\n",
        "if target not in use_df.columns:\n",
        "    raise KeyError(f\"Target column '{target}' still not present after preparation.\")\n",
        "\n",
        "categoricals = ['source_type','query_intent_category','device_type','user_country','campaign']\n",
        "for c in categoricals:\n",
        "    if c in use_df.columns:\n",
        "        use_df[c] = use_df[c].astype('category')\n",
        "\n",
        "# Separate X / y\n",
        "y = use_df[target].astype(int)\n",
        "X = use_df.drop(columns=[target])\n",
        "\n",
        "# One-hot encode categoricals (only those present)\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Basic sanity checks\n",
        "print('Feature matrix shape before split:', X.shape)\n",
        "print('Class balance:', y.value_counts(normalize=True).to_dict())\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y if y.nunique()>1 else None, random_state=42\n",
        ")\n",
        "X.shape, X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## Train XGBoost Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "clf = xgb.XGBClassifier(\n",
        "    n_estimators=160,\n",
        "    learning_rate=0.12,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    tree_method='hist',\n",
        "    random_state=42\n",
        ")\n",
        "clf.fit(X_train, y_train)\n",
        "pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, pred))\n",
        "print('Confusion Matrix:\\n', confusion_matrix(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "importances = clf.feature_importances_\n",
        "imp = pd.DataFrame({'feature': X.columns, 'importance': importances}).sort_values('importance', ascending=False).head(25)\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.barh(imp['feature'][::-1], imp['importance'][::-1])\n",
        "plt.title('Top 25 Feature Importances')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "imp.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## Summary\n",
        "- Generated synthetic human + bot clickstream data.\n",
        "- Stored each record in Redis as JSON (string values).\n",
        "- Reloaded records and trained an XGBoost fraud classifier.\n",
        "- Displayed metrics and feature importances.\n",
        "\n",
        "You can tweak DAYS / HUMANS_PER_DAY / BOTS_PER_DAY and re-run."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
