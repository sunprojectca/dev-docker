{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11658ab3",
   "metadata": {},
   "source": [
    "# Redis Click Dataset â†’ HistGradientBoosting Pipeline\n",
    "End-to-end workflow: generate synthetic click data (via FastAPI endpoint), populate Redis, load into a DataFrame, train an **HistGradientBoostingRegressor** to predict `serp_clicks`, evaluate, and inspect feature importances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36421ab",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "Adjust parameters below. Set `GENERATE = True` to trigger FastAPI to create & (optionally) insert data into Redis. \n",
    "If you've already generated data you can set it to False and just reload from Redis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c59baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "ROWS = 5000            # number of synthetic rows to generate\n",
    "DAYS = 30              # history window\n",
    "SEED = 1337            # RNG seed for reproducibility\n",
    "LEGACY = 0             # 0 = verbose schema, 1 = legacy short schema\n",
    "TO_REDIS = 1           # 1 = also push rows into Redis hashes (click:*)\n",
    "GENERATE = True        # set False to skip regeneration\n",
    "FASTAPI_BASE = 'http://localhost:8000'  # FastAPI service URL\n",
    "REDIS_HOST = 'redis'   # service hostname inside docker network\n",
    "REDIS_PORT = 6379\n",
    "TARGET = 'serp_clicks' # prediction target (for verbose schema)\n",
    "KEY_PREFIX = 'click:'  # Redis key prefix used by generator endpoint\n",
    "MAX_KEYS = None        # None for all, or set an int cap for faster experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7e2945",
   "metadata": {},
   "source": [
    "## 2. Install/Import Dependencies\n",
    "Install lightweight dependencies that might be missing (redis). The SciPy notebook image already has scikit-learn & pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd20106",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install redis requests joblib > /dev/null\n",
    "import requests, json, math, time, os\n",
    "import redis as redis_lib\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "print('Imports ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da68c7f",
   "metadata": {},
   "source": [
    "## 3. Trigger Dataset Generation via FastAPI (Optional)\n",
    "Calls the `/generate_clicks` endpoint that reuses the shared generator logic and (optionally) inserts into Redis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a0e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE:\n",
    "    params = {\n",
    "        'rows': ROWS, 'days': DAYS, 'seed': SEED, 'legacy': LEGACY, 'to_redis': TO_REDIS\n",
    "    }\n",
    "    url = f'{FASTAPI_BASE}/generate_clicks'\n",
    "    print('POST', url, params)\n",
    "    resp = requests.post(url, params=params, timeout=300)\n",
    "    if resp.status_code != 200:\n",
    "        raise RuntimeError(f'Generation failed: {resp.status_code} {resp.text}')\n",
    "    gen_info = resp.json()\n",
    "    print('Generated rows:', gen_info.get('rows'), 'CSV:', gen_info.get('path'), 'Inserted:', gen_info.get('redis_inserted'))\n",
    "else:\n",
    "    print('Skipping generation (GENERATE=False).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec081cf",
   "metadata": {},
   "source": [
    "## 4. Load Rows from Redis\n",
    "Scans Redis for keys with the chosen prefix and builds a DataFrame. For very large sets you can cap with `MAX_KEYS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd03a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = redis_lib.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)\n",
    "rows = []\n",
    "count = 0\n",
    "cursor = 0\n",
    "pattern = f'{KEY_PREFIX}*'\n",
    "while True:\n",
    "    cursor, keys = r.scan(cursor=cursor, match=pattern, count=1000)\n",
    "    for k in keys:\n",
    "        h = r.hgetall(k)\n",
    "        if h:\n",
    "            rows.append(h)\n",
    "            count += 1\n",
    "            if MAX_KEYS and count >= MAX_KEYS:\n",
    "                cursor = 0\n",
    "                break\n",
    "    if cursor == 0:\n",
    "        break\n",
    "print(f'Fetched {len(rows)} hash rows from Redis.')\n",
    "if not rows:\n",
    "    raise ValueError('No click data found in Redis. Ensure generation ran and TO_REDIS=1.')\n",
    "df = pd.DataFrame(rows)\n",
    "print('Columns:', df.columns.tolist()[:12], '... total', len(df.columns))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd79add",
   "metadata": {},
   "source": [
    "## 5. Prepare Features / Target\n",
    "We coerce numeric-looking columns, drop obvious high-cardinality identifiers, and select a target.\n",
    "If legacy schema was used adjust TARGET accordingly (e.g. `clicks`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e06578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numeric candidates\n",
    "for c in df.columns:\n",
    "    df[c] = pd.to_numeric(df[c], errors='ignore')\n",
    "# Determine target name for legacy schema if needed\n",
    "if LEGACY == 1 and TARGET == 'serp_clicks':\n",
    "    TARGET = 'clicks'\n",
    "if TARGET not in df.columns:\n",
    "    raise KeyError(f'Target {TARGET} not in DataFrame columns')\n",
    "id_like = {'user_id','session_id','landing_page_url','page_title','search_query','query','page'}\n",
    "exclude = id_like.union({TARGET})\n",
    "num_df = df.drop(columns=[c for c in df.columns if c in exclude], errors='ignore')\n",
    "X = num_df.select_dtypes(include=['number']).fillna(0)\n",
    "y = df[TARGET].astype(float)\n",
    "print('Feature cols:', len(X.columns))\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08060ed",
   "metadata": {},
   "source": [
    "## 6. Train / Evaluate HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743aa89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = HistGradientBoostingRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, pred, squared=False)\n",
    "r2 = r2_score(y_test, pred)\n",
    "print(f'RMSE: {rmse:.4f}  R2: {r2:.4f}')\n",
    "model_path = '/shared/models/hgbr_model.joblib'\n",
    "os.makedirs('/shared/models', exist_ok=True)\n",
    "joblib.dump({'model': model, 'features': list(X.columns), 'target': TARGET}, model_path)\n",
    "print('Saved model ->', model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b2ff0a",
   "metadata": {},
   "source": [
    "## 7. Feature Importance\n",
    "HistGradientBoosting exposes `feature_importances_` for interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f37e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = getattr(model, 'feature_importances_', None)\n",
    "if importances is None:\n",
    "    raise AttributeError('Model has no feature_importances_')\n",
    "fi = (pd.Series(importances, index=X.columns)\n",
    "        .sort_values(ascending=False)\n",
    "        .head(20))\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x=fi.values, y=fi.index, orient='h')\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "fi.to_frame('importance').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc244829",
   "metadata": {},
   "source": [
    "## 8. Quick Prediction Demo\n",
    "Take a small sample of test rows and compare predicted vs actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e43422",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = X_test.head(5).copy()\n",
    "pred_sample = model.predict(sample)\n",
    "compare = pd.DataFrame({\n",
    "    'actual': y_test.head(5).values,\n",
    "    'predicted': pred_sample\n",
    "})\n",
    "compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bba4f89",
   "metadata": {},
   "source": [
    "---\n",
    "### Finished\n",
    "You can adjust parameters and rerun cells 3 onward to iterate quickly."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
